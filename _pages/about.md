---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

I am Tengjun Huang (é»„è…¾éª), a third-year undergraduate at the School of Computer Science, Shandong University. My passion lies in uncovering the intricate relationships between diverse perceptual modalities and behavioral paradigms. My research interests are primarily focused on multimodal learning, computer vision, and representation learning.

My previous work has centered around computer vision and multimodal learning, delving into the intrinsic connections between a single modality and multiple other modalities. I also possess a background in video understanding, medical image segmentation, and self-supervised learning. Meanwhile, I am also interested in exploring the application of LLM-based agents in multimodal learning.

In my research, I aim to go beyond the surface-level understanding of data and instead seek to uncover deeper. Currently, an increasing number of approaches focus on scaling up data and models to address the gaps between different modalities, with the aim of learning holistic semantic connections. However, it may also be important to pay attention to underlying issues that cannot be resolved by simply enlarging the task scale. For instance, current multimodal models exhibit hallucinations during complex cross-modal reasoning and suffer significant performance drops when transferred to different domains with substantial gaps. On the other hand, we should perhaps explore more diverse learning paradigms, such as more fundamental task design, more efficient feature storage structures, more generalized optimization methods, and tighter representation coupling. This way, when mainstream development paths encounter obstacles, we still have various alternative strategies to explore. This perspective drives me to explore innovative methods and frameworks that can effectively integrate and leverage these interactions.

I am currently seeking potential PhD opportunities for Fall 2025, as well as RA or collaborative projects. If you are interested in discussing my research or exploring potential collaborations, please feel free to contact me.


# ğŸ”¥ News
- *2022.02*: &nbsp;ğŸ‰ğŸ‰ Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2022.02*: &nbsp;ğŸ‰ğŸ‰ Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
2024.06: ğŸ“ ä¸€ç¯‡è®ºæ–‡æäº¤è‡³NeurIPS 2024 (æ•°æ®é›†å’ŒåŸºå‡†)ã€‚
2024.03: ğŸ‰ My independent research on "Multimodal Representation Alignment and Efficient Transfer Learning" was accepted at ICLR 2024 workshop.
2023.11: ğŸ† Achieved 2nd place in all tracks of the NeurIPS 2023 Road Event Detection with Requirements Challenge and ç»™ oral presentation at NeurIPS 2023.
2023.06: ğŸ‰ Top 0.6% in Alibaba Cloud Tianchi Tmall Repurchase Prediction Challenge.
2022.11: ğŸ¥ˆ Awarded Second Prize in the National College Mathematical Contest.
2022.07: ğŸ¥‡ Secured National Gold Medal in the Chinese Collegiate Algorithm Design & Programming Challenge.


# ğŸ“ Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2016</div><img src='images/500x300.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Deep Residual Learning for Image Recognition](https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)

**Kaiming He**, Xiangyu Zhang, Shaoqing Ren, Jian Sun

[**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=DhtAFkwAAAAJ&citation_for_view=DhtAFkwAAAAJ:ALROH1vI_8AC) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
</div>
</div>

- [Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet](https://github.com), A, B, C, **CVPR 2020**

# ğŸ– Honors and Awards
- *2021.10* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.09* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 

# ğŸ“– Educations
- *2019.06 - 2022.04 (now)*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2015.09 - 2019.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 

# ğŸ’¬ Invited Talks
- *2021.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.03*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]](https://github.com/)

# ğŸ’» Internships
- *2019.05 - 2020.02*, [Lorem](https://github.com/), China.